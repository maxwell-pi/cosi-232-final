{
  "run_id": "9650a125-ee8f-4ad1-9949-976556502997",
  "timestamp": "2025-05-08T02-45-35.847486Z",
  "topic": "What's an embedding?",
  "seed_ids": [
    "https://openalex.org/W658020064"
  ],
  "bibliography": "The current literature reveals that embeddings are a form of representation learning that maps high-dimensional discrete input into lower-dimensional continuous vector spaces. These have been widely adopted in machine learning applications to capture domain semantics. Word2vec (W2V) and GloVe are popular word embedding algorithms that perform well across various natural language processing tasks. These embeddings reflect semantic relationships such as similarity and paraphrasing. The concept of embeddings has also been extended to graph embeddings with global graph information represented in low-dimension. Pre-trained vision-language models (VLMs) use data embeddings in compositional structures. Interestingly, embeddings exhibit seemingly linear behavior. However, comparing embeddings is a challenging task that requires systematic exploration.\n\nRecommended Papers:\n\n1. \"What the Vec? Towards Probabilistically Grounded Embeddings\" - This paper provides a theoretical understanding of what parameters in popular word embedding algorithms learn and why that is useful for downstream tasks. It also reveals an interesting mathematical interconnection between different semantic relationships.\n\n2. \"Linear Spaces of Meanings: Compositional Structures in Vision-Language Models\" - This paper investigates compositional structures in data embeddings from pre-trained VLMs. It presents a framework for understanding geometric perspective and probabilistic entailment of embeddings.\n\n3. \"GraRep++: Flexible Learning Graph Representations With Weighted Global Structural Information\" - This paper proposes a unified framework for vertex embedding that considers contributions from multiple steps. It offers an enhanced way to integrate global graph information into embedding learning.\n\n4. \"Analogies Explained: Towards Understanding Word Embeddings\" - This paper delves into the intriguing property of word embeddings where they exhibit seemingly linear behaviours. It provides a probabilistically grounded definition of paraphrasing and word transformation.\n\n5. \"Embedding Comparator: Visualizing Differences in Global Structure and Local Neighborhoods via Small Multiples\" - This paper demonstrates an interactive system for comparing embeddings. It provides a systematic way to explore differences in global structure and local neighborhoods and can be beneficial for those seeking to understand and compare different embedding models."
}